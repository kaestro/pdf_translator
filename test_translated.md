# 페이지 1

# BinaryConnect: 이진 가중치를 사용한 심층 신경망 학습

**Matthieu Courbariaux**
École Polytechnique de Montréal
matthieu.courbariaux@polymtl.ca

**Yoshua Bengio**
몬트리올 대학교, CIFAR 선임 연구원
yoshua.bengio@gmail.com

**Jean-Pierre David**
École Polytechnique de Montréal
jean-pierre.david@polymtl.ca


**요약**

심층 신경망(DNN)은 다양한 작업에서 최첨단 결과를 달성했으며, 대규모 학습 데이터셋과 대규모 모델을 사용하여 최상의 결과를 얻었습니다. 과거에는 GPU가 더 높은 계산 속도 덕분에 이러한 획기적인 발전을 가능하게 했습니다.  향후에는 학습 및 테스트 시간 모두에서 더 빠른 연산이 추가적인 발전과 저전력 장치의 소비자 애플리케이션에 매우 중요할 것입니다. 결과적으로 심층 학습(DL)을 위한 전용 하드웨어의 연구 개발에 대한 관심이 높아지고 있습니다.  즉, -1 또는 1과 같은 두 가지 값으로 제한되는 이진 가중치는 승산-누산 연산을 단순 누산으로 대체하여 특수화된 DL 하드웨어에 큰 이점을 제공합니다. 승산기는 신경망의 디지털 구현에서 가장 공간과 전력을 많이 소비하는 구성 요소이기 때문입니다.  본 논문에서는 순전파 및 역전파 중에 이진 가중치를 사용하여 DNN을 학습하는 방법인 BinaryConnect를 제시합니다.  기울기가 누적되는 저장된 가중치의 정밀도는 유지됩니다. 다른 드롭아웃 기법과 마찬가지로, BinaryConnect는 정규화 역할을 하며, 순열 불변 MNIST, CIFAR-10 및 SVHN에서 최첨단에 가까운 결과를 얻습니다.


# 1. 서론

심층 신경망(DNN)은 다양한 작업에서 최첨단 성능을 크게 향상시켰으며, 특히 음성 인식 [1, 2] 및 컴퓨터 비전(특히 이미지 객체 인식 [3, 4]) 분야에서 두드러집니다. 최근에는 심층 학습이 자연어 처리, 특히 통계적 기계 번역 [5, 6, 7] 분야에서 중요한 발전을 이루고 있습니다. 흥미롭게도 이러한 주요 발전을 가능하게 한 주요 요인 중 하나는 그래픽 처리 장치(GPU)의 등장이며, [8]부터 시작하여 분산 학습 [9, 10]을 통해 10~30배의 속도 향상을 가져왔습니다. 실제로, 더 많은 데이터로 더 큰 모델을 학습할 수 있는 능력은 지난 몇 년 동안 관찰된 획기적인 발전을 가능하게 했습니다. 오늘날 새로운 심층 학습 알고리즘과 애플리케이션을 설계하는 연구자와 개발자들은 종종 계산 능력에 제한을 받습니다. 이와 함께, GPU와 달리 저전력 장치에 심층 학습 시스템을 배치하려는 노력은 심층 신경망을 위한 특수 하드웨어의 연구 개발에 대한 관심을 크게 증가시키고 있습니다 [11, 12, 13].

심층 신경망 학습 및 적용 중 수행되는 대부분의 계산은 (인식 또는 순전파 단계에서) 실수 값 가중치에 실수 값 활성화 값을 곱하거나 (역전파 단계에서) 기울기를 곱하는 것과 관련이 있습니다. 본 논문에서는 BinaryConnect라는 접근 방식을 제안합니다.


---

# 페이지 2

# 바이너리 커넥트

이 절에서는 어떤 두 값을 선택할지, 어떻게 이산화할지, 어떻게 훈련시킬지, 그리고 어떻게 추론을 수행할지 고려하여 바이너리 커넥트에 대한 자세한 내용을 제공합니다.

## +1 또는 -1

DNN을 적용하는 것은 주로 합성곱과 행렬 곱셈으로 구성됩니다. 따라서 DL의 핵심 연산은 곱셈-누적 연산입니다. 인공 뉴런은 기본적으로 입력의 가중치 합을 계산하는 곱셈-누적기입니다.

바이너리 커넥트는 전파 중 가중치를 +1 또는 -1로 제한합니다. 결과적으로 많은 곱셈-누적 연산이 단순한 덧셈(및 뺄셈)으로 대체됩니다. 고정 소수점 가산기는 고정 소수점 곱셈-누적기에 비해 면적과 에너지 측면에서 훨씬 저렴하기 때문에 이는 큰 이점입니다 [22].

## 결정론적 이산화 대 확률적 이산화

이산화 연산은 실수 가중치를 두 가지 가능한 값으로 변환합니다. 매우 간단한 이산화 연산은 부호 함수를 기반으로 합니다.

$w_b = \begin{cases} +1 & \text{if } w \ge 0, \\ -1 & \text{otherwise} \end{cases}$ (1)

다음은 이 문서의 주요 기여입니다.

* 순전파 및 역전파 중에 이진 가중치를 사용하여 DNN을 훈련하는 방법인 바이너리 커넥트를 소개합니다(2절).
* 바이너리 커넥트가 정규화기이며 순열 불변 MNIST, CIFAR-10 및 SVHN에서 최첨단 결과에 가까운 결과를 얻는다는 것을 보여줍니다(3절).
* 바이너리 커넥트 코드를 공개합니다.

이러한 곱셈이 필요 없도록 순전파와 역전파에 사용되는 가중치를 이진으로 강제합니다. 즉, 반드시 0과 1이 아니더라도 두 값으로 제한합니다. 순열 불변 MNIST, CIFAR-10 및 SVHN에서 바이너리 커넥트를 사용하여 최첨단 결과를 얻을 수 있다는 것을 보여줍니다.

이것이 가능하게 하는 두 가지 요소는 다음과 같습니다.

1. 충분한 정밀도는 많은 확률적 기울기를 누적하고 평균내는 데 필요합니다. 하지만 노이즈가 있는 가중치(그리고 작은 수의 값으로의 이산화를 노이즈의 형태로 볼 수 있습니다. 특히 이 이산화가 확률적이라면)는 심층 학습에 대한 주요 최적화 알고리즘인 확률적 경사 하강법(SGD)과 매우 호환됩니다. SGD는 작고 노이즈가 많은 단계를 만들어 매개변수 공간을 탐색하고, 이 노이즈는 각 가중치에 누적된 확률적 기울기 기여도에 의해 평균화됩니다. 따라서 이러한 누적기에 대해 충분한 해상도를 유지하는 것이 중요합니다. 처음에는 높은 정밀도가 절대적으로 필요하다는 것을 시사하지만, [14]와 [15]는 무작위 또는 확률적 반올림을 사용하여 편향되지 않은 이산화를 제공할 수 있다는 것을 보여줍니다. [14]는 SGD가 적어도 6~8비트의 정밀도를 가진 가중치를 필요로 한다는 것을 보여주었고, [16]은 12비트 동적 고정 소수점 연산으로 DNN을 성공적으로 훈련했습니다. 게다가 뇌 시냅스의 추정 정밀도는 6~12비트입니다 [17].

2. 노이즈가 있는 가중치는 실제로 일반화에 도움이 되는 정규화의 형태를 제공합니다. 이는 변분 가중치 노이즈 [18], 드롭아웃 [19, 20] 및 드롭커넥트 [21]에서 이전에 보여진 것처럼 활성화 또는 가중치에 노이즈를 추가합니다. 예를 들어, 드롭커넥트 [21]는 바이너리 커넥트와 가장 가까우며 전파 중 가중치의 절반을 무작위로 0으로 대체하는 매우 효율적인 정규화기입니다. 이러한 이전 작업에서 알 수 있듯이 가중치의 기대값만 높은 정밀도를 가져야 하며, 노이즈는 실제로 이점이 될 수 있습니다.


---

# 페이지 3

# 페이지 3 번역

$w_b$는 이진화된 가중치이고 $w$는 실수값 가중치입니다. 이것은 결정론적 연산이지만, 은닉 유닛의 많은 입력 가중치에 걸쳐 이 이산화를 평균화하면 정보 손실을 보상할 수 있습니다. 더 정확한 평균화 과정을 위해서는 확률적으로 이진화하는 대안이 있습니다.

$w_b = \begin{cases} +1 & p = \sigma(w)일 확률 \\ -1 & 1 - p일 확률 \end{cases}$ (2)

여기서 $\sigma$는 "하드 시그모이드" 함수입니다.

$\sigma(x) = clip(\frac{x+1}{2}, 0, 1) = max(0, min(1, \frac{x+1}{2}))$ (3)

소프트 버전보다 하드 시그모이드를 사용하는 이유는 계산 비용이 훨씬 적고(소프트웨어와 특수 하드웨어 구현 모두에서), 실험에서 우수한 결과를 얻었기 때문입니다. [23]에서 제시된 "하드 tanh" 비선형성과 유사하며, 조각별 선형이고 정류기의 경계 형태에 해당합니다 [24].

## 2.3 전파 vs 업데이트

SGD 업데이트를 사용한 역전파의 여러 단계와 각 단계에서 가중치를 이산화할지 여부를 고려해 보겠습니다.

1. DNN 입력이 주어지면, 최상위 계층(입력이 주어진 DNN의 출력)으로 이어지는 계층별 유닛 활성화를 계산합니다. 이 단계를 순방향 전파라고 합니다.

2. DNN 목표가 주어지면, 첫 번째 은닉 계층까지 계층별로 위에서 아래로 내려가며 각 계층의 활성화에 대한 훈련 목표의 기울기를 계산합니다. 이 단계를 역방향 전파 또는 역방향 단계라고 합니다.

3. 각 계층의 매개변수에 대한 기울기를 계산한 다음, 계산된 기울기와 이전 값을 사용하여 매개변수를 업데이트합니다. 이 단계를 매개변수 업데이트라고 합니다.


**알고리즘 1** BinaryConnect를 사용한 SGD 훈련. C는 미니 배치에 대한 비용 함수이고, 함수 `binarize(w)`와 `clip(w)`는 가중치를 이진화하고 클리핑하는 방법을 지정합니다. L은 계층 수입니다.

**필요 조건:** (입력, 목표), 이전 매개변수 $w_{t-1}$(가중치) 및 $b_{t-1}$(편향), 그리고 학습률 $\eta$의 미니 배치.

**보장:** 업데이트된 매개변수 $w_t$ 및 $b_t$.

1. **순방향 전파:**
   $w_b \leftarrow binarize(w_{t-1})$
   k = 1부터 L까지, $a_{k-1}$, $w_b$, $b_{t-1}$을 알고 $a_k$를 계산합니다.

2. **역방향 전파:**
   출력 계층의 활성화 기울기 $\frac{\partial C}{\partial a_L}$을 초기화합니다.
   k = L부터 2까지, $\frac{\partial C}{\partial a_{k-1}}$을 $\frac{\partial C}{\partial a_k}$ 및 $w_b$를 알고 계산합니다.

3. **매개변수 업데이트:**
   $\frac{\partial C}{\partial w_b}$ 및 $\frac{\partial C}{\partial b_{t-1}}$을 $\frac{\partial C}{\partial a_k}$ 및 $a_{k-1}$을 알고 계산합니다.
   $w_t \leftarrow clip(w_{t-1} - \eta \frac{\partial C}{\partial w_b})$
   $b_t \leftarrow b_{t-1} - \eta \frac{\partial C}{\partial b_{t-1}}$


BinaryConnect를 이해하는 핵심은 순방향 및 역방향 전파(1단계 및 2단계) 동안에만 가중치를 이진화하고 매개변수 업데이트(3단계) 동안에는 이진화하지 않는다는 것입니다. 알고리즘 1에서 설명한 바와 같이, 업데이트 중에 정밀도가 높은 가중치를 유지하는 것은 SGD가 작동하는 데 필수적입니다. 이러한 매개변수 변경은 기울기 하강으로 얻어지므로 매우 작습니다. 즉, SGD는 훈련 목표(플러스 노이즈)를 가장 개선하는 방향으로 거의 무한히 작은 변화를 많이 수행합니다. 이 모든 것을 설명하는 한 가지 방법은 중요한 것이 무엇인지 가정하는 것입니다.

3


---

# 페이지 4

# 4 페이지 번역

훈련의 마지막 단계에서 가장 중요한 것은 가중치의 부호, *w*이지만, 이를 알아내기 위해 연속값을 갖는 양 *w*에 대해 여러 작은 변화를 주고, 마지막에만 부호를 고려합니다.

*w* = sign(∑<sub>t</sub> *g<sub>t</sub>*)  (4)

여기서 *g<sub>t</sub>*는  *C(f(x<sub>t</sub>, w<sub>t-1</sub>, b<sub>t-1</sub>), y<sub>t</sub>)* 의 잡음이 있는 추정값이며, *C(f(x<sub>t</sub>, w<sub>t-1</sub>, b<sub>t-1</sub>), y<sub>t</sub>)*는 (입력, 타겟) 예시 (x<sub>t</sub>, y<sub>t</sub>)에 대한 목적 함수의 값이고, w<sub>t-1</sub>은 이전 가중치이며, *w***는 가중치의 최종 이산화된 값입니다.

이러한 이산화를 잡음의 형태, 즉 정규화로 생각하는 또 다른 방법이 있습니다. 그리고 실험 결과는 이 가설을 확인합니다. 또한, 정밀도를 유지하면서 이산화 오차를 여러 가중치에서 거의 상쇄시키도록 이산화를 무작위화하여 이산화 오차를 줄일 수 있습니다. 이산화된 가중치의 기댓값을 보존하는 무작위 이산화 방식을 제안합니다.

따라서 훈련 중에 BinaryConnect는 각 미니 배치마다, 역전파의 순전파 및 역전파 단계 모두에 대해 각 가중치에 대해 두 값 중 하나를 무작위로 선택합니다. 그러나 SGD 업데이트는 매개변수를 저장하는 실수 변수에 누적됩니다.

BinaryConnect 알고리즘을 이해하는 데 도움이 되는 흥미로운 유추는 DropConnect 알고리즘 [21]입니다. BinaryConnect와 마찬가지로 DropConnect는 전파 중 가중치에만 잡음을 주입합니다. DropConnect의 잡음은 베르누이 잡음이 추가되는 반면, BinaryConnect의 잡음은 이진 샘플링 과정입니다. 두 경우 모두 손상된 값은 원래 값과 기댓값이 같습니다.

## 2.4 클리핑

이진화 연산은 크기가 이진 값 ±1을 넘어설 때 실수 가중치 *w*의 변화에 영향을 받지 않으므로, 일반적으로 가중치(일반적으로 가중치 벡터)를 경계하여 정규화하는 것이 일반적이기 때문에, 알고리즘 1에 따라 가중치 업데이트 직후 [-1, 1] 구간으로 클리핑하도록 선택했습니다. 그렇지 않으면 실수 가중치는 이진 가중치에 영향을 미치지 않고 매우 커질 것입니다.

## 2.5 추가적인 몇 가지 방법

| 최적화 | 학습률 조정 없음 | 학습률 조정 |
|---|---|---|
| SGD | 11.45% | 15.65% |
| 네스테로프 모멘텀 | 11.30% | 12.81% |
| ADAM | 10.47% |  |

표 1: 최적화 방법과 가중치 초기화 계수 [25]를 사용하여 학습률을 조정하는지 여부에 따라 (작은) CNN이 CIFAR-10에서 훈련된 테스트 오차율

모든 실험에서 내부 공변량 변화를 줄이고 가중치 척도의 전반적인 영향을 줄이기 때문에 배치 정규화(BN) [26]를 사용합니다. 또한, 모든 CNN 실험에서 ADAM 학습 규칙 [27]을 사용합니다. 마지막으로, ADAM으로 최적화할 때는 가중치 초기화 계수 [25]를 사용하여, SGD 또는 네스테로프 모멘텀 [28]으로 최적화할 때는 해당 계수의 제곱을 사용하여 가중치 학습률을 조정합니다. 표 1은 이러한 방법들의 효과를 보여줍니다.

## 2.6 테스트 시간 추론

지금까지 온더플라이 가중치 이진화를 사용하여 DNN을 훈련하는 여러 가지 방법을 소개했습니다. 훈련된 네트워크를 사용하는 합리적인 방법, 즉 새로운 예시에 대한 테스트 시간 추론을 수행하는 방법은 무엇일까요? 세 가지 합리적인 대안을 고려했습니다.

1. 결과 이진 가중치 *w<sub>b</sub>*를 사용합니다 (이는 BinaryConnect의 결정론적 형태에서 가장 합리적입니다).

4


---

# 페이지 5

# 벤치마크 결과

## 3.1 순열 불변 MNIST

MNIST는 벤치마크 이미지 분류 데이터셋입니다 [33]. 60,000개의 훈련 세트와 10,000개의 28 x 28 회색조 이미지로 구성되며, 0부터 9까지의 숫자를 나타냅니다. 순열 불변성은 모델이 데이터의 이미지 (2차원) 구조를 인식하지 못해야 함을 의미합니다 (다시 말해, CNN은 금지됩니다). 또한, 데이터 증강, 전처리 또는 비지도 사전 훈련을 사용하지 않습니다.  MNIST에서 훈련하는 MLP는 1024개의 ReLU(Rectifier Linear Units) [34, 24, 3]를 가진 3개의 은닉층과 L2-SVM 출력층으로 구성됩니다 (L2-SVM은 여러 분류 벤치마크에서 Softmax보다 성능이 우수한 것으로 나타났습니다 [30, 32]). 제곱 힌지 손실은 모멘텀 없이 SGD(확률적 경사 하강법)로 최소화됩니다. 지수적으로 감소하는 학습률을 사용합니다. 일반적으로 수행되는 방식대로, 크기 200의 미니 배치를 사용하여 훈련 속도를 높입니다.

## 표 2: MNIST, CIFAR-10, SVHN에서 DNN의 테스트 오류율

| 방법 | MNIST | CIFAR-10 | SVHN |
|---|---|---|---|
| 정규화 없음 | 1.30 ± 0.04% | 10.64% | 2.44% |
| BinaryConnect (결정적) | 1.29 ± 0.08% | 9.90% | 2.30% |
| BinaryConnect (확률적) | 1.18 ± 0.04% | 8.27% | 2.15% |
| 50% Dropout | 1.01 ± 0.04% |  |  |
| Maxout Networks [29] | 0.94% | 11.68% | 2.47% |
| Deep L2-SVM [30] | 0.87% |  |  |
| Network in Network [31] |  | 10.41% | 2.35% |
| DropConnect [21] |  |  | 1.94% |
| Deeply-Supervised Nets [32] |  | 9.78% | 1.92% |

표 2: 비지도 사전 훈련 없이 그리고 합성곱 연산 없이 MNIST, CIFAR-10 (데이터 증강 없음), SVHN에서 훈련된 DNN의 테스트 오류율 (각 방법에 따라 다름). 전파 중에 가중치당 단 하나의 비트만 사용했음에도 불구하고, 성능이 일반적인 (정규화 없는) DNN보다 실제로 더 우수하며, 특히 확률적 버전에서 그렇습니다. 이는 BinaryConnect가 정규화 역할을 한다는 것을 시사합니다.

![첫 번째 계층의 특징](IMAGE_5_그림번호)

그림 1: 정규화기에 따라 MNIST에서 훈련된 MLP의 첫 번째 계층 특징. 왼쪽에서 오른쪽으로: 정규화 없음, 결정적 BinaryConnect, 확률적 BinaryConnect, Dropout.

2. 실수 가중치 *w*를 사용합니다. 즉, 이진화는 더 빠른 훈련에는 도움이 되지만 더 빠른 테스트 시간 성능에는 도움이 되지 않습니다.

3. 확률적 경우, 각 가중치에 대해 Eq. 2에 따라 *w<sub>b</sub>*를 샘플링하여 여러 다른 네트워크를 샘플링할 수 있습니다. 이러한 네트워크의 앙상블 출력은 개별 네트워크의 출력을 평균하여 얻을 수 있습니다.

본 연구에서는 결정적 형태의 BinaryConnect를 사용하는 첫 번째 방법을 사용합니다. 확률적 형태의 BinaryConnect의 경우, 훈련 이점에 초점을 맞추고 실험에서 두 번째 방법을 사용했습니다. 즉, 실수 가중치를 사용한 테스트 시간 추론입니다. 이는 테스트 시간에 "잡음"이 제거되는 Dropout 방법의 방식을 따릅니다.

이 섹션에서는 BinaryConnect가 정규화제 역할을 하고 순열 불변 MNIST, CIFAR-10, SVHN에서 최첨단에 가까운 결과를 얻는다는 것을 보여줍니다.


---

# 페이지 6

# 그림 2: 가중치 히스토그램

![가중치 히스토그램](IMAGE_6_그림2)

규제에 따라 MNIST에서 학습된 MLP의 첫 번째 계층 가중치의 히스토그램입니다. 두 경우 모두 가중치가 결정적이 되어 훈련 오류를 줄이려고 시도하는 것처럼 보입니다. 또한, 결정적 이진 연결의 일부 가중치는 -1과 1 사이에서 주저하며 0에 머물러 있는 것으로 보입니다.


# 그림 3: 훈련 곡선

![훈련 곡선](IMAGE_6_그림3)

규제에 따라 CIFAR-10에서 CNN의 훈련 곡선입니다. 점선은 훈련 비용(제곱 힌지 손실)을, 실선은 해당 검증 오류율을 나타냅니다. 이진 연결의 두 가지 버전 모두 훈련 비용을 상당히 증가시키고, 훈련 속도를 늦추지만 검증 오류율을 낮추는데, 이는 드롭아웃 방식에서 기대할 수 있는 결과입니다.


# 3.2 CIFAR-10

CIFAR-10은 벤치마크 이미지 분류 데이터셋입니다. 50,000개의 훈련 세트와 10,000개의 테스트 세트로 구성되며, 32 x 32 컬러 이미지는 비행기, 자동차, 새, 고양이, 사슴, 개, 개구리, 말, 배, 트럭을 나타냅니다. 전역 대비 정규화와 ZCA 표백을 사용하여 데이터를 전처리하지만 데이터 증강은 사용하지 않습니다(이 데이터셋에서는 게임 체인저가 될 수 있음 [35]).  저희 CNN의 아키텍처는 다음과 같습니다.

(2 x 128C3) - MP2 - (2 x 256C3) - MP2 - (2 x 512C3) - MP2 - (2 x 1024FC) - 10SVM (5)

여기서 C3는 3 x 3 ReLU 합성곱 계층, MP2는 2 x 2 최대 풀링 계층, FC는 완전 연결 계층, SVM은 L2-SVM 출력 계층입니다. 이 아키텍처는 VGG [36]에서 크게 영감을 받았습니다. 제곱 힌지 손실은 ADAM을 사용하여 최소화됩니다. 지수적으로 감소하는 학습률을 사용합니다. 훈련 속도를 높이기 위해 미니 배치 크기 50을 사용하는 배치 정규화를 사용합니다. 훈련 세트의 마지막 5,000개 샘플을 검증 세트로 사용합니다. 최상의 검증 오류율과 관련된 테스트 오류율을 보고합니다.


---

# 페이지 7

# 3.3 SVHN

SVHN은 벤치마크 이미지 분류 데이터셋입니다. 604K개의 훈련 세트와 26K개의 32x32 컬러 이미지 테스트 세트로 구성되며, 0부터 9까지의 숫자를 나타냅니다. CIFAR-10에 사용했던 것과 동일한 절차를 따르지만, 몇 가지 주목할 만한 예외가 있습니다. 은닉 유닛 수의 절반을 사용하고, SVHN이 상당히 큰 데이터셋이기 때문에 500회 대신 200회의 에폭으로 훈련합니다. 결과는 표 2에 나와 있습니다.

# 4 관련 연구

이진 가중치를 사용하여 DNN을 훈련하는 것은 최근 연구의 주제였습니다 [37, 38, 39, 40]. 동일한 목표를 공유하지만, 우리의 접근 방식은 상당히 다릅니다. [37, 38]은 역전파(BP)가 아닌 기대 역전파(EBP)라는 변형된 방법으로 DNN을 훈련합니다. EBP는 기대 전파(EP) [41]를 기반으로 하며, 이는 확률적 그래프 모델에서 추론하는 데 사용되는 변분 베이즈 방법입니다. 우리의 방법과 그들의 방법을 비교해 보겠습니다.

* 가중치 사후 분포(이진이 아님)를 최적화합니다. 이 점에서 우리의 방법은 가중치의 실수 값 버전을 유지하기 때문에 매우 유사합니다.
* 뉴런 출력과 가중치를 모두 이진화합니다. 이는 가중치만 이진화하는 것보다 하드웨어 친화적입니다.
* 완전 연결 네트워크(MNIST에서)에 대해서는 좋은 분류 정확도를 제공하지만, 합성곱 신경망(ConvNets)에는 아직 그렇지 못합니다.

[39, 40]은 순방향 및 역방향 전파 중에 3진 가중치를 사용하여 신경망을 재훈련합니다. 즉,

* 고정밀도로 신경망을 훈련합니다.
* 훈련 후, 가중치를 -H, 0, +H의 세 가지 가능한 값으로 3진화하고, 출력 오차를 최소화하도록 H를 조정합니다.
* 그리고 마지막으로, 전파 중에는 3진 가중치로, 업데이트 중에는 고정밀 가중치로 재훈련합니다.

비교를 위해, 우리는 전파 중에 이진 가중치를 사용하여 전체적으로 훈련합니다. 즉, 우리의 훈련 절차는 순방향 및 역방향 전파의 곱셈을 피하는 효율적인 특수 하드웨어로 구현될 수 있으며, 이는 곱셈의 약 2/3에 해당합니다 (알고리즘 1 참조).

# 5 결론 및 향후 연구

우리는 순방향 및 역방향 전파 중 가중치에 대한 새로운 이진화 체계인 BinaryConnect를 제시했습니다. BinaryConnect를 사용하여 순열 불변 MNIST, CIFAR-10 및 SVHN 데이터셋에서 DNN을 훈련하고 최첨단 성능에 근접하는 것을 보여주었습니다. 이러한 방법의 영향은 특수 하드웨어 구현에서 곱셈의 약 2/3를 제거함으로써 훈련 시간을 3배까지 단축시킬 수 있습니다. BinaryConnect의 결정론적 버전을 사용하면 테스트 시간에 더욱 큰 영향을 미칠 수 있으며, 곱셈을 완전히 제거하고 메모리 요구 사항을 16비트 단정밀도에서 1비트 정밀도로 줄일 수 있습니다 (계산에 대한 메모리 비율에 영향을 미침). 향후 연구는 다른 모델과 데이터셋으로 이러한 결과를 확장하고, 훈련 중 가중치 업데이트 계산에서 곱셈을 완전히 제거하는 것을 탐구해야 합니다.


---

# 페이지 8

# 감사의 글

심도있는 논평을 주신 심사위원 여러분께 감사드립니다.  또한 도움이 되는 논의를 나누어 주신 Roland Memisevic에게도 감사드립니다.  GPU를 위한 빠르고 최적화된 코드를 쉽게 개발할 수 있도록 해준 Theano [42, 43] 개발자분들께 감사드립니다.  Theano 기반으로 구축된 두 개의 딥러닝 라이브러리인 Pylearn2 [44]와 Lasagne [45] 개발자분들께도 감사드립니다.  NSERC, 캐나다 연구 의장, Compute Canada, Nuance Foundation 및 CIFAR의 지원에도 감사드립니다.

# 참고문헌

[1] Geoffrey Hinton, Li Deng, George E. Dahl, Abdel-rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara Sainath, and Brian Kingsbury. 음성 인식에서 음향 모델링을 위한 심층 신경망.  IEEE 신호 처리 매거진, 29(6):82–97, 11월 2012.

[2] Tara Sainath, Abdel rahman Mohamed, Brian Kingsbury, and Bhuvana Ramabhadran.  LVCSR을 위한 심층 합성곱 신경망. ICASSP 2013, 2013.

[3] A. Krizhevsky, I. Sutskever, and G. Hinton. 심층 합성곱 신경망을 이용한 ImageNet 분류. NIPS'2012, 2012.

[4] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich.  합성곱을 이용한 심층화. 기술 보고서, arXiv:1409.4842, 2014.

[5] Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas Lamar, Richard Schwartz, and John Makhoul. 통계적 기계 번역을 위한 빠르고 견고한 신경망 결합 모델. Proc. ACL'2014, 2014.

[6] Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 신경망을 이용한 시퀀스-투-시퀀스 학습. NIPS'2014, 2014.

[7] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 정렬 및 번역을 공동으로 학습하는 신경 기계 번역. ICLR'2015, arXiv:1409.0473, 2015.

[8] Rajat Raina, Anand Madhavan, and Andrew Y. Ng. 그래픽 프로세서를 사용한 대규모 심층 비지도 학습. ICML'2009, 2009.

[9] Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Jauvin. 신경 확률 언어 모델. 기계 학습 연구 저널, 3:1137-1155, 2003.

[10] J. Dean, G.S Corrado, R. Monga, K. Chen, M. Devin, Q.V. Le, M.Z. Mao, M.A. Ranzato, A. Senior, P. Tucker, K. Yang, and A. Y. Ng. 대규모 분산 심층 신경망. NIPS'2012, 2012.

[11] Sang Kyun Kim, Lawrence C McAfee, Peter Leonard McMahon, and Kunle Olukotun. 고도로 확장 가능한 제한된 볼츠만 머신 FPGA 구현. 현장 프로그래밍 가능 논리 및 응용 프로그램, 2009. FPL 2009. 국제 학술대회, 367-372 페이지. IEEE, 2009.

[12] Tianshi Chen, Zidong Du, Ninghui Sun, Jia Wang, Chengyong Wu, Yunji Chen, and Olivier Temam. Diannao: 유비쿼터스 기계 학습을 위한 소형 고처리량 가속기. 프로그래밍 언어 및 운영 체제에 대한 아키텍처 지원에 관한 제19회 국제 학술대회 논문집, 269-284 페이지. ACM, 2014.

[13] Yunji Chen, Tao Luo, Shaoli Liu, Shijin Zhang, Liqiang He, Jia Wang, Ling Li, Tianshi Chen, Zhiwei Xu, Ninghui Sun, et al. Dadiannao: 기계 학습 슈퍼컴퓨터. 마이크로 아키텍처 (MICRO), 2014년 제47회 연례 IEEE/ACM 국제 심포지엄, 609-622 페이지. IEEE, 2014.

[14] Lorenz K Muller and Giacomo Indiveri. 저해상도 시냅스 가중치를 위한 신경망 라운딩 방법. arXiv 사전 인쇄 arXiv:1504.05767, 2015.

[15] Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan. 제한된 수치 정밀도를 사용한 심층 학습. ICML'2015, 2015.

[16] Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. 심층 학습을 위한 저정밀 산술. Arxiv: 1412.7024, ICLR'2015 워크숍, 2015.

[17] Thomas M Bartol, Cailey Bromer, Justin P Kinney, Michael A Chirillo, Jennifer N Bourne, Kristen M Harris, and Terrence J Sejnowski. 해마 가시 머리 크기는 매우 정확하다. bioRxiv, 2015.

[18] Alex Graves. 신경망을 위한 실용적인 변분 추론. J. Shawe-Taylor, R.S. Zemel, P.L. Bartlett, F. Pereira, and K.Q. Weinberger 편집, 신경 정보 처리 시스템 24, 2348-2356 페이지. Curran Associates, Inc., 2011.

[19] Nitish Srivastava. 드롭아웃을 이용한 신경망 개선. 석사 논문, 토론토 대학교, 2013.


---

# 페이지 9

# 참고문헌

[20] 니티쉬 스리바스타바, 제프리 힌턴, 알렉스 크리제브스키, 일리아 슷스케버, 그리고 루슬란 살라후디노프. 드롭아웃: 과적합을 방지하는 간단한 방법. 기계 학습 연구 저널, 15:1929-1958, 2014.

[21] 리 완, 매튜 자일러, 쉬신 장, 얀 르쿤, 그리고 롭 퍼거스. 드롭커넥트를 이용한 신경망의 규제화. ICML'2013, 2013.

[22] J.P. 데이비드, K. 칼라치, 그리고 N. 티틀리. 모듈러 곱셈과 지수승의 하드웨어 복잡도. 컴퓨터, IEEE 트랜잭션, 56(10):1308-1319, 2007년 10월.

[23] R. 콜로베르. 대규모 기계 학습. 파리 6대학 LIP6 박사 학위 논문, 2004.

[24] X. 글로롯, A. 보르데스, 그리고 Y. 벵지오. 심층 희소 정류기 신경망. AISTATS 2011, 2011.

[25] 자비에 글로롯 그리고 요슈아 벵지오. 심층 순전파 신경망 훈련의 어려움에 대한 이해. AISTATS 2010, 2010.

[26] 세르게이 이오페 그리고 크리스티안 세제디. 배치 정규화: 내부 공변량 이동을 줄임으로써 심층 신경망 훈련 가속화. 2015.

[27] 디데릭 킹마 그리고 지미 바. 아담: 확률적 최적화를 위한 방법. arXiv 논문 arXiv:1412.6980, 2014.

[28] 유 네스테로프. 수렴 속도 o(1/k²)을 가진 무제약 볼록 최소화 문제를 위한 방법. 도클라디 AN SSSR (소비에트 수학 도클.으로 번역됨), 269:543-547, 1983.

[29] 이안 J. 굿펠로우, 데이비드 워드-퍼즐리, 메디 미르자, 아론 쿠르빌 그리고 요슈아 벵지오. 맥스아웃 네트워크. 기술 보고서 Arxiv 보고서 1302.4389, 몬트리올 대학교, 2013년 2월.

[30] 이추안 탕. 선형 서포트 벡터 머신을 이용한 심층 학습. 표현 학습에 대한 워크숍, ICML, 2013.

[31] 민 린, 치앙 첸, 그리고 슈이청 얀. 네트워크 안의 네트워크. arXiv 논문 arXiv:1312.4400, 2013.

[32] 첸-유 리, 사이닝 시에, 패트릭 갤러거, 정유 장, 그리고 주오웬 투. 심층 감독 네트워크. arXiv 논문 arXiv:1409.5185, 2014.

[33] 얀 르쿤, 레온 보투, 요슈아 벵지오, 그리고 패트릭 해프너. 문서 인식에 적용된 기울기 기반 학습. IEEE 논문집, 86(11):2278-2324, 1998년 11월.

[34] V. 나이르 그리고 G.E. 힌턴. 정류 선형 유닛은 제한된 볼츠만 머신을 개선합니다. ICML'2010, 2010.

[35] 벤자민 그레이엄. 공간적으로 희소한 합성곱 신경망. arXiv 논문 arXiv:1409.6070, 2014.

[36] 카렌 시모니안 그리고 앤드류 지서먼. 대규모 이미지 인식을 위한 매우 심층적인 합성곱 신경망. ICLR, 2015.

[37] 다니엘 사우드리, 이타이 후바라, 그리고 론 마이어. 기대치 역전파: 연속 또는 불연속 가중치를 가진 다층 신경망의 매개변수 없는 훈련. NIPS'2014, 2014.

[38] 지용 청, 다니엘 사우드리, 젝시 마오, 그리고 전중 란. 기대치 역전파를 이용한 이미지 분류를 위한 이진 다층 신경망 훈련. arXiv 논문 arXiv:1503.03562, 2015.

[39] 큐예온 황 그리고 원용 숭. 가중치 +1, 0, 그리고 -1을 사용하는 고정 소수점 순전파 심층 신경망 설계. 신호 처리 시스템 (SiPS), 2014 IEEE 워크숍, 1-6 페이지. IEEE, 2014.

[40] 종홍 킴, 큐예온 황, 그리고 원용 숭. 피드포워드 심층 신경망을 사용한 X1000 실시간 음소 인식 VLSI. 음향, 음성 및 신호 처리 (ICASSP), 2014 IEEE 국제 컨퍼런스, 7510-7514 페이지. IEEE, 2014.

[41] 토마스 P. 민카. 근사 베이지안 추론을 위한 기대치 전파. UAI'2001, 2001.

[42] 제임스 버그스트라, 올리비에 브뢰뢰, 프레데릭 바스티앙, 파스칼 람블린, 라즈반 파스카누, 기욤 데자르댕, 조셉 투리안, 데이비드 워드-퍼즐리, 그리고 요슈아 벵지오. 테아노: CPU 및 GPU 수학 표현식 컴파일러. 과학 컴퓨팅 컨퍼런스를 위한 파이썬 컨퍼런스 (SciPy) 논문집, 2010년 6월. 구두 발표.

[43] 프레데릭 바스티앙, 파스칼 람블린, 라즈반 파스카누, 제임스 버그스트라, 이안 J. 굿펠로우, 아르노 베르제롱, 니콜라 부샤르, 그리고 요슈아 벵지오. 테아노: 새로운 기능과 속도 개선. 심층 학습 및 비지도 특징 학습 NIPS 2012 워크숍, 2012.

[44] 이안 J. 굿펠로우, 데이비드 워드-퍼즐리, 파스칼 람블린, 빈센트 두몰린, 메디 미르자, 라즈반 파스카누, 제임스 버그스트라, 프레데릭 바스티앙, 그리고 요슈아 벵지오. 파이러닝2: 기계 학습 연구 라이브러리. arXiv 논문 arXiv:1308.4214, 2013.

[45] 산더 디엘레만, 얀 슐터, 콜린 라펠, 에번 올슨, 스렌 카에 슨더비, 다니엘 누리, 다니엘 마투라나, 마틴 토마, 에릭 배텐버그, 잭 켈리, 제프리 드 포, 마이클 하일만, 디오고149, 브라이언 맥피, 헨드릭 와이데만, 타카스그84, 피터데리바즈, 존, 인스타깁스, 카시프 라술 박사, 콩리우, 브라이트퓨리, 그리고 조나스 드그레이브. 라자냐: 첫 출시, 2015년 8월.

9
